{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import isbnlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_files = {\n",
    "    \"editions\": {\n",
    "        \"file_path\": os.path.join(data_base_path, \"editions_after_2015.txt\"),\n",
    "        \"total_lines\": 4259552,\n",
    "    },\n",
    "    \"works\": {\n",
    "        \"file_path\": os.path.join(data_base_path, \"filtered_works_with_editions.txt\"),\n",
    "        \"total_lines\": 909883,\n",
    "    },\n",
    "    \"authors\": {\n",
    "        \"file_path\": os.path.join(data_base_path, \"filtered_authors_with_works_and_editions.txt\"),\n",
    "        \"total_lines\": 853086,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to ask shell scripts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path, total_lines, valid_cols):\n",
    "    \"\"\"\n",
    "    Reads a tab-separated text file and extracts JSON data from the fifth column, \n",
    "    filtering it based on valid column keys.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Path to the input text file.\n",
    "    total_lines : int\n",
    "        Estimated total number of lines in the file (used for tqdm progress bar).\n",
    "    valid_cols : list of str\n",
    "        List of valid keys to extract from the JSON data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where each key is a line index and the value is a dictionary \n",
    "        containing the extracted JSON fields that match `valid_cols`.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_dict = {}\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for idx, line in enumerate(tqdm(infile, total=total_lines, desc=\"Processing lines\")):\n",
    "            # if idx > 10000:\n",
    "            #     break\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            \n",
    "            if len(parts) < 5:\n",
    "                continue \n",
    "\n",
    "            try:\n",
    "                json_data = json.loads(parts[4])\n",
    "                \n",
    "                for key in json_data.keys():\n",
    "                    df_dict[idx] = {key: json_data.get(key, np.nan) for key in valid_cols if key in json_data}\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_type = \"editions\"\n",
    "\n",
    "ed_cols = [\n",
    "    \"key\", \"works\", \"title\", \"publishers\", \"description\", \"first_sentence\", \"subjects\", \"languages\",\n",
    "    \"publish_date\", \"publish_country\", \"number_of_pages\", \"latest_revision\", \"revision\"\n",
    "]\n",
    "\n",
    "file_path, total_lines = extracted_files[record_type][\"file_path\"], extracted_files[record_type][\"total_lines\"]\n",
    "editions_dict = read_txt(file_path, total_lines, ed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editions_df = pd.DataFrame.from_dict(editions_dict, orient=\"index\")\n",
    "editions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del editions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editions_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editions_df.isna().sum() / len(editions_df) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep cols with below 70% nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols = [\"key\", \"works\", \"title\", \"publishers\", \"publish_date\", \"languages\", \"subjects\"]\n",
    "\n",
    "drp_df = editions_df[final_cols]\n",
    "del editions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drp_df[\"publishers\"] = drp_df[\"publishers\"].str[0]\n",
    "drp_df[\"publishers\"].fillna(\"Self-published\", inplace=True)\n",
    "drp_df[\"publish_year\"] = ([int(x[:4]) for x in drp_df[\"publish_date\"]])\n",
    "drp_df[\"subjects\"] = drp_df[\"subjects\"].str.join(\", \")\n",
    "drp_df[\"works\"] = drp_df[\"works\"].apply(lambda x: x[0][\"key\"] if isinstance(x, list) and x else None)\n",
    "drp_df[\"languages\"] = drp_df[\"languages\"].apply(\n",
    "    lambda x: x[0][\"key\"].split(\"/\")[-1] if isinstance(x, list) and x and \"key\" in x[0] else np.nan\n",
    ")\n",
    "\n",
    "final_editions_df = drp_df.dropna(subset=[\"title\", \"languages\", \"subjects\"])\n",
    "\n",
    "del drp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_editions_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_editions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_editions_df[\"text\"] = final_editions_df[[\"title\", \"subjects\"]].astype(str).agg(\" \".join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "east_asian_editions_df = final_editions_df[final_editions_df[\"languages\"].isin([\"jpn\", \"kor\", \"chi\"])]\n",
    "east_asian_editions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/stsb-xlm-r-multilingual\")\n",
    "embeddings = model.encode(east_asian_editions_df[\"text\"].tolist(), batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svd = TruncatedSVD(n_components=min(embeddings.shape) - 1)\n",
    "X_svd = svd.fit_transform(embeddings)\n",
    "\n",
    "# Determine components for 90% variance\n",
    "explained_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "n_components = np.searchsorted(explained_variance, 0.90) + 1\n",
    "print(f\"Number of components to retain 90% variance: {n_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=n_components)\n",
    "X_svd = svd.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = pd.DataFrame(X_svd, columns=[f\"SV {i+1}\" for i in range(X_svd.shape[1])])\n",
    "X_final[\"publish_year\"] = east_asian_editions_df[\"publish_year\"]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], s = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "east_asian_editions_df[\"dbscan_cluster\"] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=east_asian_editions_df[\"dbscan_cluster\"], cmap=\"Spectral\", alpha=0.5)\n",
    "plt.colorbar()\n",
    "plt.title(\"Book Clusters Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, metric=\"euclidean\", cluster_selection_method=\"eom\")\n",
    "east_asian_editions_df[\"hdbscan_cluster\"] = clusterer.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "linkage_matrix = sch.linkage(X_scaled, method=\"ward\")\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sch.dendrogram(linkage_matrix, truncate_mode=\"level\", p=5)\n",
    "plt.title(\"Ward - Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Book Samples\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "east_asian_editions_df[\"hierarchical_cluster\"] = sch.fcluster(linkage_matrix, t=120, criterion=\"distance\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=east_asian_editions_df[\"hierarchical_cluster\"])\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Ward Linkage Clustering\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
